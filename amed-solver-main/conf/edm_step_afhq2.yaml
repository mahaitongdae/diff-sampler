hydra:
  run:
    dir: ./exps/${now:%Y-%m-%d}/${now:%H-%M-%S}-${dataset_name}-${env.num_steps}

seed: 1
runner_class_name: 'DiffSamplerOnPolicyRunner'
dataset_name: 'afhqv2'
afs: False
desc: None
m: 1
nosubdir: False
outdir: './exps'
dry_run: False
device: 'cuda'
empirical_normalization: False
save_interval: 100
logger: 'wandb'
wandb_project: 'rl4diffusion'

model:
  guidance_type: None
  guidance_rate: None
  
env:
  batch_size: 256
  num_steps: 10
  schedule_type: 'polynomial'
  schedule_rho: 7
  # reward_scale: 1
  reward_type: 'variance'
  reward_scale: 0.1
  reward_scale_terminal: 0.1
  scale_dir: 0.01
  r_range:
    low: 0.2
    high: 0.4
  
policy:
  class_name: 'DiffSamplerActorCritic'
  init_noise_std: 0.2
  hidden_dim: 128
  output_dim: 1
  bottleneck_input_dim: 64
  bottleneck_output_dim: 4
  noise_channels: 8
  embedding_type: 'positional'
  dataset_name: 'afhqv2'
  img_resolution: 64
  num_steps: 50
  M: None
  guidance_type: None
  guidance_rate: None
  schedule_type: None
  schedule_rho: None
  scale_dir: 0
  scale_time: 0
  max_order: None
  predict_x0: True
  lower_order_final: True
  
algorithm:

  value_loss_coef: 1.0
  use_clipped_value_loss: True
  clip_param: 0.2
  entropy_coef: 0.01
  num_learning_epochs: 5
  num_mini_batches: 4  # mini batch size: num_envs*nsteps / nminibatches
  learning_rate: 1.e-3  # 5.e-4
  schedule: 'adaptive'  # could be adaptive fixed
  gamma: 0.99
  lam: 0.95
  desired_kl: 0.01
  max_grad_norm: 1.
  
runner:
  policy_class_name: 'DiffSamplerActorCritic'
  alg_class_name: 'PPO'
  algorithm_class_name: 'PPO'
  num_steps_per_env: 24  # per iteration
  max_iterations: 50  # number of policy updates
  
  # logging
  save_interval: 50  # check for potential saves every this many iterations
  experiment_name: 'test'
  run_name: ''
  # load and resume
  resume: False
  load_run: -1  # -1: last run
  checkpoint: -1  # -1: last saved model
  resume_path: None  # updated from load_run and chkpt
  